{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/melbourne-housing-snapshot/melb_data.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost\n\nBy definition, **ensemble methods** combine the predictions of several models\n\n**Gradient boosting** is a method that goes through cycles to iteratively add models into an ensemble.\n\n![Gradient boosting](https://i.imgur.com/MvCGENh.png)"},{"metadata":{},"cell_type":"markdown","source":"## Example"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Read the data\ndata = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n\n# Select subset of predictors\ncols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = data[cols_to_use]\n\n# Select target\ny = data.Price\n\n# Separate data into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"   Rooms  Distance  Landsize  BuildingArea  YearBuilt\n0      2       2.5     202.0           NaN        NaN\n1      2       2.5     156.0          79.0     1900.0\n2      3       2.5     134.0         150.0     1900.0\n3      3       2.5      94.0           NaN        NaN\n4      4       2.5     120.0         142.0     2014.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rooms</th>\n      <th>Distance</th>\n      <th>Landsize</th>\n      <th>BuildingArea</th>\n      <th>YearBuilt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2.5</td>\n      <td>202.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2.5</td>\n      <td>156.0</td>\n      <td>79.0</td>\n      <td>1900.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2.5</td>\n      <td>134.0</td>\n      <td>150.0</td>\n      <td>1900.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2.5</td>\n      <td>94.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2.5</td>\n      <td>120.0</td>\n      <td>142.0</td>\n      <td>2014.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"XGBoost stands for extreme gradient boosting, which is an implementation of gradient boosting with several additional features focused on performance and speed. (Scikit-learn has another version of gradient boosting, but XGBoost has some technical advantages.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# build and fit a model just as we would in scikit-learn\nfrom xgboost import XGBRegressor\n\nmy_model = XGBRegressor()\nmy_model.fit(X_train, y_train)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n             min_child_weight=1, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n             validate_parameters=False, verbosity=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions and evaluate the model\nfrom sklearn.metrics import mean_absolute_error\n\npredictions = my_model.predict(X_valid)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))","execution_count":7,"outputs":[{"output_type":"stream","text":"Mean Absolute Error: 237002.5216506351\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Parameter Tuning\n\nXGBoost has a few parameters that can dramatically affect accuracy and training speed:"},{"metadata":{},"cell_type":"markdown","source":"### n_estimators\n\n*n_estimators* specifies how many times to go through the modeling cycle described above. It is equal to the number of models that we include in the ensemble.\n\n* Too low a value causes **underfitting**, which leads to inaccurate predictions on both training data and test data.\n* Too high a value causes **overfitting**, which causes accurate predictions on training data, but inaccurate predictions on test data (which is what we care about).\n\nTypical values range from 100-1000, though this depends a lot on the *learning_rate*"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model = XGBRegressor(n_estimators=500)\nmy_model.fit(X_train, y_train)\npredictions = my_model.predict(X_valid)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))","execution_count":8,"outputs":[{"output_type":"stream","text":"Mean Absolute Error: 245361.2754855486\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### early_stopping_rounds\n\n*early_stopping_rounds* offers a way to automatically find the ideal value for *n_estimators*. \n\nEarly stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for *n_estimators*. It's smart to set a high value for *n_estimators* and then use *early_stopping_rounds* to find the optimal time to stop iterating.\n\nSetting ***early_stopping_rounds*=5 is a reasonable choice**. In this case, we stop after 5 straight rounds of deteriorating validation scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model = XGBRegressor(n_estimators=500)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)],\n             verbose=False)\npredictions = my_model.predict(X_valid)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))","execution_count":11,"outputs":[{"output_type":"stream","text":"Mean Absolute Error: 242505.29704068482\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"If you want to fit a model with all of your data, set *n_estimators* to whatever value you found to be optimal when run with early stopping."},{"metadata":{},"cell_type":"markdown","source":"### learning_rate\n\nIn general, a small learning rate and large number of estimators will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle. As default, XGBoost sets ***learning_rate*=0.1**."},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\npredictions = my_model.predict(X_valid)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))","execution_count":15,"outputs":[{"output_type":"stream","text":"Mean Absolute Error: 241684.17751058543\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### n_jobs\n\nOn larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter *n_jobs* equal to the number of cores on your machine. On smaller datasets, this won't help.\n\nThe resulting model won't be any better, so micro-optimizing for fitting time is typically nothing but a distraction. But, it's useful in large datasets where you would otherwise spend a long time waiting during the fit command."},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\npredictions = my_model.predict(X_valid)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}